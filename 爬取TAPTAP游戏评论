# -*- coding: utf-8 -*-
"""
Created on Sun Jul 28 15:48:56 2019

@author: liuwp
"""


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# 混合方法——正则表达式+直接搜标签 
###---------------
# 时间用正则表达式获取  用户名、评分、评论用标签获取，最后拼接
# 需人为修改的设定的地方如下（用【】表示）   
##【游戏编号】TAPTAP对应的游戏编号，例如山海镜花是137744
##【排列顺序】TAPTAP评论排序，有update/hot/spent三种属性，分别对应 按时间排序/按热度排序/按游戏时长排序
##【爬取页数】一共需要获取多少页评论，一页有20个评论
 ##【保存地址】存在哪里，以xlsx结尾
###-------------
from bs4 import BeautifulSoup
import requests
import re
import unicodedata
import pandas as pd

GameNumber = 137744      ##【游戏编号】
CommentPage = 3     ##【爬取页数】
CommentOrder = 'update'  ##【排列顺序】update/hot/spent
SaveAddress = 'E:/SHJH_gamecomments1.xlsx'  ##【保存地址】以xlsx结尾

def getHtmlText(url):
    try:
        r = requests.get(url,timeout = 30, )
        r.raise_for_status()
        r.encoding = 'utf-8'
        return r.text
    except:
        print("Error")

def parText(result,text):
    name_search = text.find("span", class_='taptap-user').get_text()
    score = int(int(text.find("div", class_='item-text-score').find("i", class_='colored').get("style")[-4:-2])//14)
    comment = text.find("div", class_='item-text-body').get_text().strip()
    
    result.append((name_search, score, comment))

def get_comment_date(soup):

    comment_header = soup.select(".review-item-text .item-text-header")
    pattern_date='data-dynamic-time=".*?">(.*?)</span>'                    
    date=re.findall(pattern_date, str(comment_header), re.S)              
    
    return date

if __name__ == '__main__':
    url1 = 'https://www.taptap.com/app/'
    url2 = '/review?order='
    url3 = '&page='
    url4 = '#review-list'
    result1 = []
    for count in range(1,CommentPage+1):
        try:  # 如果某个页面出错则继续爬取下一页
            result = []
            date_out = []
            url = url1 + str(GameNumber) + url2 + str(CommentOrder) + url3 +str(count) + url4
            html = getHtmlText(url)
            soup = BeautifulSoup(html, 'html.parser')
            date_tmp=get_comment_date(soup)  #获取评论时间
            date_out.extend(date_tmp)
            for i in soup.find_all('div', class_='review-item-text'):
                parText(result, i)
                print("\r进度:{:2f}%".format(count * 100 / CommentPage), end="") #打印进度
            for j in range(len(date_out)):
                    result1.append([result[j][0],date_out[j],result[j][1],result[j][2]])  #字段的排序
            
        except Exception as e:
            print("\r进度:{:2f}%".format(count*100/CommentPage),end="")
            print()
            print(str(e))
pd.DataFrame(result1, columns=['用户名','评论时间', '评分值', '评论']).to_excel(SaveAddress, index=None, encoding='utf-8')  # 【指定字段写入的规则】以sep的属性作为分隔符，将文件写入txt文件中


###---------------
#方法1——正则表达式方法
#获取用户名字的正则表达式写不出来，目前可以获取 评分，内容以及时间
###-------------
#import pandas as pd
#import numpy as np
#import requests
#import re
#from bs4 import BeautifulSoup
#import time as tm
#
#url_1="https://www.taptap.com/app/137744/review?order=update&page="        	#url开头
#url_2="#review-list"                                                       	#url结尾
#page_total=1                                                               	#需要爬取的总页数
#
#name_out=[]
#comment_out=[]                                                         		#输出容器
#score_out=[]
#date_out=[]
#
#def get_comment_name(star_bs):
#    comment = star_bs.select(".review-item-test .item-text-header")
#    pattern_name = 'data-taptap-url.*?class.*?rel.*?>(.*?)</a>'
#    name = re.findall(pattern_name, str(comment), re.S)
#    
#    return name
#
#def get_comment_text(star_bs):
#
#    comment = star_bs.select(".review-item-text ")                         	#选中目标class，将从这个class中匹配内容
#    pattern_pinglun = '<div class.*?data-review.*?"contents">(.*?)</div>'  	#建立正则表达式，(.*?)是最后会截取出来的内容	
#    pinglun = re.findall(pattern_pinglun, str(comment), re.S)              	#调用re库进行正则匹配，保存结果
#      
#    return pinglun
#
#def get_comment_score(star_bs):
#
#    comment = star_bs.select(".review-item-text ")                         
#    pattern_score='<i class.*?"width: (.*?)px"></i>'                     
#    score = re.findall(pattern_score, str(comment), re.S)             
#    score_num = [ int(x)/14 for x in score ]                               #抓出来的是字符串，转化为整型，并计算评分
#    
#    return score_num
#

#
#for j in range(1,page_total+1):  						   #总共爬5页数据
#    t1=tm.time()
#    pagenum=str(j)
#    link=url_1+pagenum+url_2 						   #拼接每一页的url
#    
#    star = requests.get(link) 						   #加载url
#    star_bs = BeautifulSoup(star.text, "html.parser")  #把url对象转化为美味汤
#    
#    name_tmp=get_comment_name(star_bs)
#    comment_tmp=get_comment_text(star_bs)              #获取评论
#    score_tmp=get_comment_score(star_bs)               #获取分数
#    date_tmp=get_comment_date(star_bs)                 #获取评论时间
    
    
#    name_out.extend(name_tmp)
#    comment_out.extend(comment_tmp)                    #装入输出容器
#    score_out.extend(score_tmp)
#    date_out.extend(date_tmp)
#    t2=tm.time()
#    timing=t2-t1                                       #计时，用于调试
#    print('page %d grapped, %5.2f seconds used' % (j,timing))  #输出爬取进度
#
#result={"Account" : name_out,
#        "Date" : date_out,
#        "Score" : score_out,
#        "Comment" : comment_out}                     #先把列表转为字典
#
#resultpd=pd.DataFrame(result)                          #再把字典转为pandas数据框   
#resultpd.to_excel('E:\SHJHgamecomments.xlsx')             	   #输出excel文件

###---------------
#方法2——直接搜索字段方法
#遇到修改过的评论会报错从而跳过这一页，需要改进时间的获取方法
###-----------
#from bs4 import BeautifulSoup
#import requests
##import re
#import unicodedata
#import pandas as pd
#
#def getHtmlText(url):
#    try:
#        r = requests.get(url,timeout = 30, )
#        r.raise_for_status()
#        r.encoding = 'utf-8'
#        return r.text
#    except:
#        print("Error")
#
#def parText(result,text):
#    name_search = text.find("span", class_='taptap-user').get_text()
#    score = int(70//int(text.find("div", class_='item-text-score').find("i", class_='colored').get("style")[-4:-2]))
#    comment = text.find("div", class_='item-text-body').get_text().strip()
#    time_search = text.find("a", class_='text-header-time').find_all("span")
#    for span in time_search:
#        if span.get('title'):
#            time = span.get("title")[4:]
#    result.append((name_search, time, score, comment))
#
#if __name__ == '__main__':
#    start_url = 'https://www.taptap.com/app/137744/review?order=update&page='  #【爬取网页指定】 可根据类型、时间、标签筛选出来的新URL进行筛选，统一加上&page=,作为start_url
#    result = []
#    depth = 1
#    for count in range(1,depth+1):
#        try:  # 如果某个页面出错则继续爬取下一页
#            url = start_url + str(count)
#            html = getHtmlText(url)
#            soup = BeautifulSoup(html, 'html.parser')
#            for i in soup.find_all('div', class_='review-item-text'):
#                parText(result, i)
#                print("\r进度:{:2f}%".format(count * 100 / depth), end="") #打印进度
#        except Exception as e:
#            print("\r进度:{:2f}%".format(count*100/depth),end="")
#            print()
#            print(str(e))
#pd.DataFrame(result, columns=['用户名', '评论时间', '评分值', '评论']).to_excel('E:/gamereply.xlsx', index=None, encoding='utf-8')  # 【指定字段写入的规则】以sep的属性作为分隔符，将文件写入txt文件中
